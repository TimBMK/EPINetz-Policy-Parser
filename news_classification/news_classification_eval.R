## Initial Classification ##
############################

# Classify Tweets #

{
  library(tidyverse)
  library(quanteda)
  library(quanteda.textstats)
  library(data.table)
  library(furrr)
  library(vroom)
  library(igraph)
  library(RandomWalkRestartMH)
  library(scales)
  library(patchwork)
}

source("utils_text_processing.R")
source("classify_documents.R")


dir = "news_classification"

list.files(file.path(dir, "walk_terms")) %>% str_remove(".csv") # all available timeframes

rwr_timeframe = "2020-03-29" # Time frame of the RWR - for testing purposes


## settings

classification_timeframe = weeks(1) # length of the timeframe for the classification, e.g. one week before the RWR timeframe
classification_before_after = "before" # set if the classification_timeframe is before or after the rwr_timeframe, e.g. the week before or after. "after" to for after, "before" for before


classification_measure = "ScoreNormMean" # one of ScoreNorm, ScoreNormMean, ScoreNormGroup, ScoreNormGroupMean, or (for raw scores) Score or ScoreMean

classification_cutoff = NULL # Should an additional cutoff be set? Applies to classification measure. NULL to skip

seedterm_value = NULL # Should Seed Term Scores values be set to a fixed value for classification? NULL to skip. Otherwise enter a numerical value. Applies to classification_measure only

keep_seed_terms = NULL
                    
cut_frequent_policy_terms = NULL  # Should terms appearing in numerous policy field be cut? 
                                  #  "auto" to cut terms appearing in more than 50% of the policy fields
                                  #  numeric value for a specific number
                                  #  NULL to skip

cutoff_value = 0.5 # a numerical value to set. Scores below will be set to 0. NULL to skip

cutoff_quantile = FALSE # if TRUE, the cutoff_value specifies a quantile, rather than a fixed value

cutoff_normalized_scores = TRUE # if TRUE, the cutoff is applied to the normalized scores. Otherwise, normalization is applied after the cutoff

minimum_results = 50 # Numerical minimum number of results for each group to be returned. Bypasses the cutoff_value as needed. NULL to skip

normalize_scores = "doc" # should the score in the documents be normalized between 0 and 1? Can be "doc" (normalize within each document), "group" (normalize for each group), or NULL to skip
      # normalizing the scores per document seems beneficial for longer docs, "group" is better suited for short documents like tweets

return_walk_terms = TRUE # should the processed walk terms be returned for further analysis and transparency?

return_unclassified_docs = TRUE # should the IDs of the unlassified docs be returned? 




# Classify Documents

## read in data

walk_terms <- vroom(paste0(dir, "/walk_terms/", rwr_timeframe, ".csv"))

news_docs <- read_timelimited_data(file = file.path(dir, "data/data_news_2019-2021.csv.tar.gz"),
                                   guess_max = 10000,
                                   filter_var = "_source.estimated_date",
                                   starting_point = rwr_timeframe,
                                   timeframe = classification_timeframe,
                                   before_after = classification_before_after)


classification_NE <- list.files(dir, 
                                pattern = paste0("tokens_news_", 
                                                 str_extract(rwr_timeframe, 
                                                             "\\d+(?=-)")), # only read the required tokens object
                                full.names = TRUE) %>% 
  vroom() %>% 
  inner_join(news_docs %>% select(`_id`), by = join_by(doc_id == `_id`)) %>% # keep only the required tokens, as determined by the doc IDs in the time-filtered docs object
  filter_tokens(tokens_col = "lemma", 
                tags = c("NN", "NE"), # Noun words and NEs only
                #minimum string length, stopwords dictionaries, additional stopwords and lower casing set to default
                replies = NULL, # not needed
                keep_mentions = NULL, # not needed
                keep_urls = NULL) # not needed





# Classify
classification_result <- classify_documents(
  walk_terms, # data with terms generated by get_rwr_terms()
  group_name = "policy_field", # name of the group variable which specifies the classes the documents will be sorted into
  document_tokens = classification_NE, # data with the documents to be classified. Expects tokenized data, with a doc_id and one token per row
  tokens_var = "lemma", # name of the tokens variable within the documents dataframe. Usually "tokens", "lemma", etc.
  doc_id = "doc_id", # name of the doc_id variable in document_tokens
  classification_measure = classification_measure, # set the measure to use for classification here. Must be present in the data
  classification_cutoff = classification_cutoff, # Should a cutoff be set to filter the walk_terms?? Applies to classification measure. NULL to skip. This is useful if a more strict cutoff is desired than in get_rwr_terms()
  keep_seed_terms = keep_seed_terms, # should seed terms be kept even if their score is lower than the cutoff? only applies if a cutoff is specified
  seedterm_value = seedterm_value, # Should Seed Term Scores values be set to a fixed value for classification? NULL to skip. Otherwise enter a numerical value. Applies to classification_measure only
  normalize_scores = normalize_scores, # should the score in the documents be normalized between 0 and 1? Can be doc (normalize within each document), group (normalize for each group), or NULL to skip
  cutoff_value = cutoff_value, # a numerical value to set. Scores below will be set to 0. NULL to skip
  cutoff_quantile = cutoff_quantile, # if TRUE, the cutoff_value specifies a quantile, rather than a fixed value
  cutoff_normalized_scores = cutoff_normalized_scores, # if TRUE, the cutoff is applied to the normalized scores. Otherwise, normalization is applied after the cutoff
  minimum_results = minimum_results, # Numerical minimum number of results for each group to be returned. Bypasses the cutoff_value as needed. NULL to skip  
  cut_frequent_group_terms = cut_frequent_policy_terms,
  return_walk_terms = return_walk_terms, # should the processed walk terms be returned for further analysis and transparency?
  return_unclassified_docs = return_unclassified_docs, # should the IDs of the unlassified docs be returned? 
  # setting return_walk_terms or return_unclassified_docs to TRUE returns a list of dataframes rather than a single dataframe
  verbose = TRUE
  )



## check results

top_group_terms(classification_result,
                group_name = "policy_field",
                classification_measure = classification_measure,
                print_seed_terms = TRUE,
                n = 50, # set number of terms to return here
                mode = "print") # should the results merely be printed (print) or returned as a data object (return)?


top_docs <- top_group_documents(classification_result,
                                documents = news_docs,
                                doc_id = join_by(doc_id == `_id`),
                                group_name = "policy_field",
                                classification_score = "score_norm", # this can be switched for "score" to view non-normalized top docs
                                n = 20, # set number of documents to return here
                                with_ties = TRUE, # should all results with the same score be kept if there is a tie?
                                mode = "return") # should the results merely be printed (print) or returned as a data object (return)?

unclassified_docs <- get_unclassified_documents( 
  classification_result, # the result of classify_documents(). requires the walk_terms data
  documents = news_docs, # the full document data to be matched to the classification result for printout
  doc_id = join_by(doc_id == `_id`), # the name of the doc_id used for matching. Can be a join_by() function where classification_result = a and documents = b
  n = 20, # the number of documents to print/return 
  mode = "return" # should the results be printed out a dataframe of results be returned?
) 
